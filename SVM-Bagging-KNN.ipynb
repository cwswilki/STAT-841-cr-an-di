{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1302cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2333ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3\n",
      "No dataframe.  Loading data...\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-42-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-48-1conn.log.labeled.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307685/1639896004.py:25: DtypeWarning: Columns (8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframes.append(pd.read_csv(full_path, sep =\"|\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-8-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-44-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-9-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv\n",
      "Using file: /home/craig-wilkinson/.cache/kagglehub/datasets/agungpambudi/network-malware-detection-connection-analysis/versions/3/CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"agungpambudi/network-malware-detection-connection-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "RELOAD_DATA = False\n",
    "if not RELOAD_DATA:\n",
    "  try:\n",
    "    print(df.head())\n",
    "  except Exception as e:\n",
    "    print(\"No dataframe.  Loading data...\")\n",
    "    RELOAD_DATA=True\n",
    "if RELOAD_DATA:\n",
    "  dataframes = []\n",
    "  import os\n",
    "  for dirname, _, filenames in os.walk(path):\n",
    "    for index, filename in enumerate(filenames):\n",
    "      full_path = os.path.join(dirname, filename)\n",
    "      print(f\"Using file: {full_path}\")\n",
    "      dataframes.append(pd.read_csv(full_path, sep =\"|\"))\n",
    "  df = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e29e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SKIPPED_COLUMNS = [\n",
    "  'ts', 'uid', 'id.orig_h', 'id.resp_h', 'tunnel_parents', 'detailed-label']\n",
    "ONE_HOT_COLUMNS = ['proto', 'service', 'conn_state', 'local_orig', 'local_resp', 'history', ]\n",
    "NUMERIC_COLUMNS = [\n",
    "   'id.orig_p', 'id.resp_p', #??\n",
    "   'duration', 'orig_bytes',\n",
    "     'resp_bytes', 'missed_bytes', \n",
    "   'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes',\n",
    "]\n",
    "LABEL_COLUMN = ['label']\n",
    "\n",
    "def process_data(df, sample_count, test_pct):\n",
    "  df = df[ONE_HOT_COLUMNS + NUMERIC_COLUMNS + LABEL_COLUMN]\n",
    "\n",
    "  if sample_count:\n",
    "    df = df.sample(n=sample_count, random_state=42).copy()\n",
    "  for col in NUMERIC_COLUMNS:\n",
    "    df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.loc[:, col] = df[col].fillna(-1)\n",
    "\n",
    "  df_test = df.sample(frac=test_pct)\n",
    "  features_test = df_test.drop(columns=LABEL_COLUMN)\n",
    "  y_test = pd.DataFrame(index=df_test.index)\n",
    "  y_test['label'] = np.where(df_test[LABEL_COLUMN[0]] == 'Benign', 1, 0)\n",
    "  \n",
    "  df_train = df.drop(df_test.index)\n",
    "  features_train = df_train.drop(columns=LABEL_COLUMN)\n",
    "  y_train = pd.DataFrame(index=df_train.index)\n",
    "  \n",
    "  #y_train['label'] = (1 if df_train[LABEL_COLUMN[0]] == 'Benign' else 0)\n",
    "  y_train['label'] = np.where(df_train[LABEL_COLUMN[0]] == 'Benign', 1, 0)\n",
    "\n",
    "  return (\n",
    "      features_train.reset_index(drop=True), \n",
    "      y_train.reset_index(drop=True), \n",
    "      features_test.reset_index(drop=True), \n",
    "      y_test.reset_index(drop=True)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a5329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9ca443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Question 1 - b\n",
    "from numpy.random.mtrand import f\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train_svm(df_features_train, df_y_train, C=1, kernel=\"linear\"):\n",
    "  preprocessor = make_column_transformer(\n",
    "      (OneHotEncoder(handle_unknown='ignore', sparse_output=False), ONE_HOT_COLUMNS),\n",
    "      (MinMaxScaler(), NUMERIC_COLUMNS),\n",
    "      remainder='passthrough'\n",
    "  )\n",
    "  preprocessor.fit(df_features_train)\n",
    "  print(f\"Total input features (original): {df_features_train.shape[1]}\")\n",
    "  feature_names = preprocessor.get_feature_names_out()\n",
    "  print(f\"Total output features (post-encoding): {len(feature_names)}\")\n",
    "  # Train the model\n",
    "  classifier = make_pipeline(\n",
    "      preprocessor,\n",
    "      SVC(kernel=kernel,\n",
    "          C=C\n",
    "          )\n",
    "  )\n",
    "  classifier.fit(df_features_train, df_y_train)\n",
    "\n",
    "  return classifier\n",
    "\n",
    "def train_bagging(df_features_train, df_y_train, max_depth=2, max_trees=50):\n",
    "\n",
    "  preprocessor = make_column_transformer(\n",
    "      (OneHotEncoder(handle_unknown='ignore'), ONE_HOT_COLUMNS),\n",
    "      remainder='passthrough'\n",
    "  )\n",
    "  tree_classifier = sklearn.tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "  bagging_classifier = make_pipeline(\n",
    "      preprocessor,\n",
    "      sklearn.ensemble.BaggingClassifier(\n",
    "          estimator=tree_classifier,\n",
    "          n_estimators=max_trees,\n",
    "          max_samples=0.5,\n",
    "          bootstrap=False,\n",
    "          n_jobs=4,\n",
    "          random_state=RANDOM_STATE)\n",
    "  )\n",
    "  bagging_classifier.fit(df_features_train, df_y_train)\n",
    "\n",
    "  return bagging_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53552618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import sys\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "VERBOSE = False\n",
    "class ClassifierBase:\n",
    "    def __init__(self, df_features_train, df_y_train):\n",
    "        usable_numeric_columns = NUMERIC_COLUMNS.copy()\n",
    "        # Any numeric column with only one value, will result in division by zero\n",
    "        # during normalization, and should just be removed.\n",
    "        for col in NUMERIC_COLUMNS:\n",
    "            if df_features_train[col].max(axis=0) == df_features_train[col].min(axis=0):\n",
    "                if VERBOSE:\n",
    "                    print(\"DROPPING COL!!!!!!!!!!!!!!\")\n",
    "                    print(col)\n",
    "                df_features_train = df_features_train.drop(columns=[col])\n",
    "                usable_numeric_columns.remove(col)\n",
    "\n",
    "        self.df_features_train = df_features_train\n",
    "        self.df_y_train = df_y_train\n",
    "        self.preprocessor = make_column_transformer(\n",
    "            (OneHotEncoder(handle_unknown='ignore'), ONE_HOT_COLUMNS),\n",
    "            (MinMaxScaler(), usable_numeric_columns),\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        self.preprocessor.fit(df_features_train)\n",
    "        feature_names = self.preprocessor.get_feature_names_out()\n",
    "        if VERBOSE:\n",
    "            print(f\"Final feature names: {feature_names}\")\n",
    "    def preprocess(self, df_features):\n",
    "        X_transformed = self.preprocessor.transform(df_features)\n",
    "        print(f\"Global feature space dimensionality: {X_transformed.shape[1]}\")\n",
    "        return X_transformed\n",
    "    \n",
    "class SvmKMeansClassifier(ClassifierBase, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, df_features_train, df_y_train):\n",
    "        super().__init__(df_features_train, df_y_train)\n",
    "        self.classifier = None\n",
    "    \n",
    "    def transform_kmeans(self, features, labels, kmeans_clusters_per_class):\n",
    "        features_train_svm = []\n",
    "        y_train_svm = []\n",
    "\n",
    "        unique_labels = np.unique(labels[\"label\"])\n",
    "        print(unique_labels)\n",
    "        for label in unique_labels:\n",
    "            # Filter data for this specific class\n",
    "            class_data = features[(labels['label'] == label).values]\n",
    "\n",
    "            kmeans = MiniBatchKMeans(\n",
    "                n_clusters=kmeans_clusters_per_class, \n",
    "                batch_size=4096,\n",
    "                random_state=42,\n",
    "                n_init=\"auto\"\n",
    "            )\n",
    "        \n",
    "            kmeans.fit(class_data)\n",
    "            centers = kmeans.cluster_centers_\n",
    "\n",
    "            # The cluster centers become our new \"training examples\"\n",
    "            features_train_svm.append(centers)\n",
    "            y_train_svm.append(np.full(centers.shape[0], label))\n",
    "\n",
    "        features_final = np.vstack(features_train_svm)\n",
    "        labels_final = np.concatenate(y_train_svm)\n",
    "        return features_final, labels_final\n",
    "\n",
    "\n",
    "    def compute_metrics(self, X, y):\n",
    "        assert self.classifier is not None\n",
    "        X_transformed = self.preprocess(X)\n",
    "        \n",
    "        # We don't apply k-means here, because this was just a training trick.\n",
    "        # KMeans always produces a dense feature array, so we force our data\n",
    "        # here to be dense to match.\n",
    "        dense_X_transformed = X_transformed.toarray()\n",
    "        predictions = self.classifier.predict(dense_X_transformed)\n",
    "        \n",
    "        return classification_report(y, predictions)\n",
    "\n",
    "    def train(self, df_features, df_y, C, kernel, kmeans_clusters_per_class):\n",
    "        self.kmeans_clusters_per_class = kmeans_clusters_per_class\n",
    "        X_transformed = self.preprocess(df_features)\n",
    "        X_final, y_final = self.transform_kmeans(X_transformed, df_y, kmeans_clusters_per_class)\n",
    "        print(X_final.shape)\n",
    "\n",
    "\n",
    "        self.classifier = SVC(\n",
    "            kernel=kernel,\n",
    "            C=C,\n",
    "        )\n",
    "        \n",
    "        self.classifier.fit(X_final, y_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d261570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self, df_features_all):\n",
    "        usable_numeric_columns = NUMERIC_COLUMNS.copy()\n",
    "        # Any numeric column with only one value, will result in division by zero\n",
    "        # during normalization, and should just be removed.\n",
    "        for col in NUMERIC_COLUMNS:\n",
    "            if df_features_all[col].max(axis=0) == df_features_all[col].min(axis=0):\n",
    "                if VERBOSE:\n",
    "                    print(\"DROPPING COL!!!!!!!!!!!!!!\")\n",
    "                    print(col)\n",
    "                df_features_all = df_features_all.drop(columns=[col])\n",
    "                usable_numeric_columns.remove(col)\n",
    "\n",
    "        self.preprocessor = make_column_transformer(\n",
    "            (OneHotEncoder(handle_unknown='ignore'), ONE_HOT_COLUMNS),\n",
    "            (MinMaxScaler(), usable_numeric_columns),\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        self.preprocessor.fit(df_features_all)\n",
    "        if VERBOSE:\n",
    "            feature_names = self.preprocessor.get_feature_names_out()\n",
    "            print(f\"Final feature names: {feature_names}\")\n",
    "\n",
    "    def process_features(self, df_features):\n",
    "        X_transformed = self.preprocessor.transform(df_features)\n",
    "        return X_transformed\n",
    "\n",
    "class ClassifierBaseFOR_BAGGING:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SvmKMeansClassifierFOR_BAGGING(ClassifierBaseFOR_BAGGING, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C, kernel, kmeans_clusters_per_class):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.kernel=kernel\n",
    "        self.kmeans_clusters_per_class = kmeans_clusters_per_class\n",
    "    \n",
    "    def transform_kmeans(self, features, labels, kmeans_clusters_per_class):\n",
    "        features_train_svm = []\n",
    "        y_train_svm = []\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "        print(unique_labels)\n",
    "        for label in unique_labels:\n",
    "            # Filter data for this specific class\n",
    "            mask = (labels == label)\n",
    "            \n",
    "            # Apply mask to X. \n",
    "            # Note: This works for both Dense Arrays and Sparse Matrices\n",
    "            class_data = features[mask]\n",
    "\n",
    "            kmeans = MiniBatchKMeans(\n",
    "                n_clusters=kmeans_clusters_per_class, \n",
    "                batch_size=4096,\n",
    "                random_state=42,\n",
    "                n_init=\"auto\"\n",
    "            )\n",
    "        \n",
    "            kmeans.fit(class_data)\n",
    "            centers = kmeans.cluster_centers_\n",
    "\n",
    "            # The cluster centers become our new \"training examples\"\n",
    "            features_train_svm.append(centers)\n",
    "            y_train_svm.append(np.full(centers.shape[0], label))\n",
    "\n",
    "        features_final = np.vstack(features_train_svm)\n",
    "        labels_final = np.concatenate(y_train_svm)\n",
    "        return features_final, labels_final\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert self.classifier is not None\n",
    "        \n",
    "        # We don't apply k-means here, because this was just a training trick.\n",
    "        # KMeans always produces a dense feature array, so we force our data\n",
    "        # here to be dense to match.\n",
    "        dense_X_transformed = X.toarray()\n",
    "        predictions = self.classifier.predict(dense_X_transformed)\n",
    "        return predictions\n",
    "\n",
    "    # matching base\n",
    "    def fit(self, X, y):\n",
    "        # To improve expressiveness across the bag of elements, we pick\n",
    "        # values for these parameters randomly.\n",
    "        chosen_cluster_count = random.choice(self.kmeans_clusters_per_class)\n",
    "        chosen_C = random.choice(self.C)\n",
    "        chosen_kernel = random.choice(self.kernel)\n",
    "        # Next, since the dataset is too large anyways, we extract different\n",
    "        # subsamples for each classifier.\n",
    "        print(f\"Training SVM with: Clusters:{chosen_cluster_count} - C:{chosen_C} - Kernel: {chosen_kernel}\")\n",
    "\n",
    "        # Inside bagging this is now a matrix, not a dataframe\n",
    "        #X, y = self.subsample(X, y, self.subsample_count)\n",
    "\n",
    "        X_final, y_final = self.transform_kmeans(X, y, chosen_cluster_count)\n",
    "        print(X_final.shape)\n",
    "\n",
    "        self.classifier = SVC(\n",
    "            kernel=chosen_kernel,\n",
    "            C=chosen_C,\n",
    "        )\n",
    "        \n",
    "        self.classifier.fit(X_final, y_final)\n",
    "\n",
    "    def compute_metrics(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        print(classification_report(y, predictions))  \n",
    "\n",
    "    def subsample(self, X, y, count):\n",
    "        X_subsample = X.sample(n=count)\n",
    "        y_subsample = y.loc[X_subsample.index]\n",
    "        return X_subsample, y_subsample\n",
    "\n",
    "\n",
    "def train_svm_bagging(df_features_train, df_y_train,\n",
    "                      C, kernel, kmeans_clusters_per_class,\n",
    "                      subsample_pct, estimator_count):\n",
    "    svm_classifier = SvmKMeansClassifierFOR_BAGGING(\n",
    "        C, kernel, kmeans_clusters_per_class\n",
    "    )\n",
    "\n",
    "    bagging_classifier = make_pipeline(\n",
    "        sklearn.ensemble.BaggingClassifier(\n",
    "            estimator=svm_classifier,\n",
    "            n_estimators=estimator_count,\n",
    "            max_samples=subsample_pct,\n",
    "            bootstrap=False,\n",
    "            n_jobs=4,\n",
    "            random_state=RANDOM_STATE)\n",
    "    )\n",
    "    bagging_classifier.fit(df_features_train, df_y_train)\n",
    "\n",
    "    return bagging_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2ac777",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # with multiprocessing.Pool(processes=self.n_estimators) as pool:\n",
    "    #         # starmap unpacks the tuple arguments into the function args\n",
    "    #         self.estimators = pool.starmap(_train_single_estimator, tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1e8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier # For classification\n",
    "\n",
    "def train_knn(k, df_features_train, df_y_train):\n",
    "    preprocessor = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown='ignore'), ONE_HOT_COLUMNS),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    knn_classifier = make_pipeline(\n",
    "        preprocessor,\n",
    "        KNeighborsClassifier(n_neighbors = k + 1)\n",
    "    )\n",
    "    knn_classifier.fit(df_features_train, df_y_train)\n",
    "\n",
    "    return knn_classifier\n",
    "#   y_preds = knn.predict(test_X)\n",
    "\n",
    "#   correct = 0\n",
    "#   incorrect = 0\n",
    "#   for single_y_test, y_pred in zip(test_y, y_preds):\n",
    "#     if single_y_test == y_pred:\n",
    "#       correct += 1\n",
    "#     else:\n",
    "#       incorrect += 1\n",
    "#   return correct / (correct + incorrect)\n",
    "\n",
    "# def train_and_evaluate_sklearn_knn(train_X, train_y, test_X, test_y):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     kfold_dataset_parts = []\n",
    "#     max_train_samples = 0\n",
    "#     for split in splits:\n",
    "#         train_indices, validation_indices = split\n",
    "\n",
    "#         train_dataset_tr = [train_dataset[i] for i in train_indices]\n",
    "#         train_dataset_val = [train_dataset[i] for i in validation_indices]\n",
    "#         train_X = [x for y, x in train_dataset_tr]\n",
    "#         train_y = [y for y, x in train_dataset_tr]\n",
    "#         val_X = [x for y, x in train_dataset_val]\n",
    "#         val_y = [y for y, x in train_dataset_val]\n",
    "#         kfold_dataset_parts.append((train_X, train_y, val_X, val_y))\n",
    "#         max_train_samples = max(max_train_samples, len(train_dataset_tr))\n",
    "\n",
    "#     splits = kf.split(train_dataset)\n",
    "#     best_accuracy = 0\n",
    "#     for k in range(1, max_train_samples - 1):\n",
    "#         accuracies = []\n",
    "#         for train_X, train_y, val_X, val_y in kfold_dataset_parts:\n",
    "#             accuracies.append(train_and_evaluate_sklearn_knn(\n",
    "#                 k, train_X, train_y, val_X, val_y))\n",
    "\n",
    "#         accuracy = sum(accuracies) / len(accuracies)\n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_k = k\n",
    "#         print(f\"Average cross-validation (k={k}): {accuracy}\")\n",
    "\n",
    "#     print(f\"Best N-Fold Validation Accuracy: {best_accuracy}\")\n",
    "#     print(f\"Best K: {best_k}\")\n",
    "\n",
    "#     test_accuracy = train_and_evaluate_sklearn_knn(\n",
    "#         best_k,\n",
    "#         [x for y, x in train_dataset],\n",
    "#         [y for y, x in train_dataset],\n",
    "#         [x for y, x in test_dataset],\n",
    "#         [y for y, x in test_dataset])\n",
    "\n",
    "#     print(f\"Final test accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a24af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307685/1407199442.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[:, col] = df[col].fillna(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        proto service conn_state local_orig local_resp history  id.orig_p  \\\n",
      "0         tcp       -         S0          -          -       S    58968.0   \n",
      "1         tcp       -         S0          -          -       S    39102.0   \n",
      "2         tcp       -         S0          -          -       S    48178.0   \n",
      "3         tcp       -         S0          -          -       S    42564.0   \n",
      "4         tcp       -     RSTOS0          -          -       I    55982.0   \n",
      "...       ...     ...        ...        ...        ...     ...        ...   \n",
      "4949995   tcp       -         S0          -          -       S    40606.0   \n",
      "4949996   tcp       -     RSTOS0          -          -       I    32045.0   \n",
      "4949997   tcp       -        OTH          -          -       C    45589.0   \n",
      "4949998   tcp       -         S0          -          -       S    21187.0   \n",
      "4949999   tcp       -         S0          -          -       S    38723.0   \n",
      "\n",
      "         id.resp_p  duration orig_bytes resp_bytes  missed_bytes  orig_pkts  \\\n",
      "0             23.0      -1.0       -1.0       -1.0           0.0        1.0   \n",
      "1             23.0      -1.0       -1.0       -1.0           0.0        1.0   \n",
      "2             23.0  3.140466        0.0        0.0           0.0        3.0   \n",
      "3             23.0  0.000002        0.0        0.0           0.0        2.0   \n",
      "4             80.0  2.688476        0.0        0.0           0.0        2.0   \n",
      "...            ...       ...        ...        ...           ...        ...   \n",
      "4949995       23.0      -1.0       -1.0       -1.0           0.0        1.0   \n",
      "4949996       80.0  3.725138        0.0        0.0           0.0        4.0   \n",
      "4949997    62336.0      -1.0       -1.0       -1.0           0.0        0.0   \n",
      "4949998       23.0      -1.0       -1.0       -1.0           0.0        1.0   \n",
      "4949999       81.0      -1.0       -1.0       -1.0           0.0        1.0   \n",
      "\n",
      "         orig_ip_bytes  resp_pkts  resp_ip_bytes  \n",
      "0                 60.0        0.0            0.0  \n",
      "1                 60.0        0.0            0.0  \n",
      "2                180.0        0.0            0.0  \n",
      "3                120.0        0.0            0.0  \n",
      "4                 80.0        0.0            0.0  \n",
      "...                ...        ...            ...  \n",
      "4949995           40.0        0.0            0.0  \n",
      "4949996          160.0        0.0            0.0  \n",
      "4949997            0.0        0.0            0.0  \n",
      "4949998           40.0        0.0            0.0  \n",
      "4949999           40.0        0.0            0.0  \n",
      "\n",
      "[4950000 rows x 16 columns]\n",
      "         label\n",
      "0            1\n",
      "1            1\n",
      "2            1\n",
      "3            0\n",
      "4            0\n",
      "...        ...\n",
      "4949995      0\n",
      "4949996      0\n",
      "4949997      0\n",
      "4949998      0\n",
      "4949999      0\n",
      "\n",
      "[4950000 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/craig-wilkinson/miniconda3/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:930: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with: Clusters:30 - C:10.0 - Kernel: linear\n",
      "[0 1]\n",
      "Training SVM with: Clusters:300 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:30 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:3000 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(60, 181)\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:300 - C:10.0 - Kernel: linear\n",
      "[0 1]\n",
      "Training SVM with: Clusters:3000 - C:1.0 - Kernel: linear\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:30 - C:0.01 - Kernel: linear\n",
      "[0 1]\n",
      "(60, 181)\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:3000 - C:0.1 - Kernel: linear\n",
      "[0 1]\n",
      "Training SVM with: Clusters:300 - C:0.01 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:1.0 - Kernel: linear\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:30 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:3000 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(6000, 181)\n",
      "(6000, 181)\n",
      "Training SVM with: Clusters:300 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:300 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: linear\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:3000 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(6000, 181)\n",
      "Training SVM with: Clusters:300 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "Training SVM with: Clusters:300 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:0.01 - Kernel: linear\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:3000 - C:0.01 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:30 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:300 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:30 - C:10.0 - Kernel: linear\n",
      "[0 1]\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:3000 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "Training SVM with: Clusters:300 - C:0.01 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: linear\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(6000, 181)\n",
      "Training SVM with: Clusters:300 - C:1.0 - Kernel: linear\n",
      "[0 1]\n",
      "Training SVM with: Clusters:30 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:3000 - C:10.0 - Kernel: poly\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:0.01 - Kernel: linear\n",
      "[0 1]\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:300 - C:0.1 - Kernel: linear\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "(6000, 181)\n",
      "(600, 181)\n",
      "Training SVM with: Clusters:30 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(60, 181)\n",
      "Training SVM with: Clusters:3000 - C:0.1 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n",
      "Training SVM with: Clusters:3000 - C:1.0 - Kernel: poly\n",
      "[0 1]\n",
      "(6000, 181)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def compute_metrics(classifier, df_features, df_y):\n",
    "  predictions = classifier.predict(df_features)\n",
    "  \n",
    "  return classification_report(df_y, predictions)\n",
    "\n",
    "class KnnConfig:\n",
    "  def __init__(self,\n",
    "               K):\n",
    "    self.K = K\n",
    "\n",
    "class SvmConfig:\n",
    "  def __init__(self,\n",
    "               C,\n",
    "               kernels):\n",
    "    self.C = C\n",
    "    self.kernels = kernels\n",
    "\n",
    "class BaggingConfig:\n",
    "  def __init__(self,\n",
    "               max_depth,\n",
    "               max_trees):\n",
    "    self.max_depth = max_depth\n",
    "    self.max_trees = max_trees\n",
    "\n",
    "class SvmBaggingConfig:\n",
    "  def __init__(self,\n",
    "               C,\n",
    "               kernels,\n",
    "               cluster_counts,\n",
    "               max_submodels,\n",
    "               subsample_pct):\n",
    "    self.C = C\n",
    "    self.kernels = kernels\n",
    "    self.cluster_counts = cluster_counts\n",
    "    self.max_submodels = max_submodels\n",
    "    self.subsample_pct = subsample_pct\n",
    "\n",
    "\n",
    "class Config:\n",
    "  def __init__(self,\n",
    "               svm_config:SvmConfig = None,\n",
    "               bagging_config:BaggingConfig = None,\n",
    "               knn_config:KnnConfig = None,\n",
    "               svm_bagging_config:SvmBaggingConfig = None,\n",
    "               sample_count = None,\n",
    "               test_pct = 0.2):\n",
    "    self.svm_config = svm_config\n",
    "    self.bagging_config = bagging_config\n",
    "    self.knn_config = knn_config\n",
    "    self.svm_bagging_config = svm_bagging_config\n",
    "    self.sample_count = sample_count\n",
    "    self.test_pct = test_pct\n",
    "\n",
    "last_sample_count = -1\n",
    "last_test_pct = -1\n",
    "\n",
    "# Allows defining a list of configs for a long-running\n",
    "# training batch, where multiple approaches are run without\n",
    "# Adjusting configs.\n",
    "try:\n",
    "  test = final_results[0]\n",
    "except Exception as e:\n",
    "  final_results = []\n",
    "\n",
    "for config in [\n",
    "    # Config(\n",
    "    #   svm_config=SvmConfig(\n",
    "    #       C=[0.01, 0.1, 1.0, 10.0],\n",
    "    #       kernels=['poly', 'linear', 'rbf', 'sigmoid']\n",
    "    #   ),\n",
    "    #   sample_count=10_000,\n",
    "    #   test_pct=0.5,\n",
    "    # ),\n",
    "    # Config(\n",
    "    #   knn_config=KnnConfig(\n",
    "    #     K=300\n",
    "    #   ),\n",
    "    #   sample_count=100_000,\n",
    "    #   test_pct=0.1,\n",
    "    # ),\n",
    "    # Config(\n",
    "    #   bagging_config=BaggingConfig(\n",
    "    #     max_depth=5,\n",
    "    #     max_trees=50\n",
    "    #   ),\n",
    "    #   sample_count=None,\n",
    "    #   test_pct=0.2,\n",
    "    # ),\n",
    "    Config(\n",
    "      svm_bagging_config=SvmBaggingConfig(\n",
    "          C=[0.01, 0.1, 1.0, 10.0],\n",
    "          kernels=['poly', 'linear'],\n",
    "          cluster_counts=[30, 300, 3000],\n",
    "          max_submodels=40,\n",
    "          subsample_pct=0.02\n",
    "      ),\n",
    "      sample_count=5_500_000,\n",
    "      test_pct=0.1,\n",
    "    ),\n",
    "    ]:\n",
    "  print(config.sample_count)\n",
    "  if last_sample_count != config.sample_count or last_test_pct != config.test_pct:\n",
    "    last_sample_count = config.sample_count\n",
    "    last_test_pct = config.test_pct\n",
    "    df_features_train, df_y_train, df_features_test, df_y_test = process_data(df, config.sample_count,\n",
    "                                                                              config.test_pct)\n",
    "    print(df_features_train)\n",
    "    print(df_y_train)\n",
    "    \n",
    "  # TODO - add support for KFold cross validation\n",
    "  # TODO - add support for GridSearchCV\n",
    "  if config.bagging_config:\n",
    "    bagging_classifier = train_bagging(df_features_train, df_y_train,\n",
    "                                        max_depth=config.bagging_config.max_depth,\n",
    "                                        max_trees=config.bagging_config.max_trees)\n",
    "    metrics = compute_metrics(bagging_classifier, df_features_test, df_y_test)\n",
    "    print(metrics)\n",
    "    final_results.append((bagging_classifier, metrics, config))\n",
    "  elif config.svm_bagging_config:\n",
    "    preprocessor = PreProcessor(df_features_all=df_features_train)\n",
    "    X_transformed = preprocessor.process_features(df_features_train)\n",
    "\n",
    "    bagging_classifier = train_svm_bagging(\n",
    "        X_transformed, df_y_train,\n",
    "        config.svm_bagging_config.C,\n",
    "        config.svm_bagging_config.kernels,\n",
    "        config.svm_bagging_config.cluster_counts,\n",
    "        config.svm_bagging_config.subsample_pct,\n",
    "        config.svm_bagging_config.max_submodels)\n",
    "\n",
    "    X_test = preprocessor.process_features(df_features_test)\n",
    "    metrics = compute_metrics(bagging_classifier, X_test, df_y_test)\n",
    "    print(metrics)\n",
    "    final_results.append((bagging_classifier, metrics, config))\n",
    "  elif config.svm_config:\n",
    "    # svm_classifier = train_svm(df_features_train, df_y_train)\n",
    "    for kernel in config.svm_config.kernels:\n",
    "      for c in config.svm_config.C:\n",
    "        for cluster_count in [20, 100, 400, 1000]:\n",
    "          print(f\"Experiment: {kernel} - {c} - clust: {cluster_count}\")\n",
    "          classifier = SvmKMeansClassifier(df_features_train, df_y_train)\n",
    "          classifier.train(df_features_train, df_y_train, c, kernel,\n",
    "                           kmeans_clusters_per_class=cluster_count)\n",
    "          metrics = classifier.compute_metrics(df_features_test, df_y_test)\n",
    "          print(metrics)\n",
    "          final_results.append((classifier, metrics, config))\n",
    "\n",
    "  elif config.knn_config:\n",
    "    knn_classifier = train_knn(config.knn_config.K, df_features_train, df_y_train)\n",
    "    metrics = compute_metrics(knn_classifier, df_features_test, df_y_test)\n",
    "    print(metrics)\n",
    "    final_results.append((knn_classifier, metrics, config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26dfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.72      0.79      6476\n",
      "           1       0.61      0.81      0.70      3524\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.74      0.77      0.74     10000\n",
      "weighted avg       0.78      0.75      0.76     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99   3247654\n",
      "           1       0.97      1.00      0.98   1754547\n",
      "\n",
      "    accuracy                           0.99   5002201\n",
      "   macro avg       0.98      0.99      0.99   5002201\n",
      "weighted avg       0.99      0.99      0.99   5002201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a, b, c in final_results:\n",
    "    print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
